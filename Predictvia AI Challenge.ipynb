{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-01T02:46:29.670695Z",
     "iopub.status.busy": "2020-12-01T02:46:29.669742Z",
     "iopub.status.idle": "2020-12-01T02:46:29.675644Z",
     "shell.execute_reply": "2020-12-01T02:46:29.674849Z"
    },
    "papermill": {
     "duration": 0.031563,
     "end_time": "2020-12-01T02:46:29.675777",
     "exception": false,
     "start_time": "2020-12-01T02:46:29.644214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/aichallenge2020/training.csv\n",
      "/kaggle/input/aichallenge2020/sampleSubmission.csv\n",
      "/kaggle/input/aichallenge2020/test.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-01T02:46:29.714165Z",
     "iopub.status.busy": "2020-12-01T02:46:29.713329Z",
     "iopub.status.idle": "2020-12-01T02:46:29.868300Z",
     "shell.execute_reply": "2020-12-01T02:46:29.867655Z"
    },
    "papermill": {
     "duration": 0.176773,
     "end_time": "2020-12-01T02:46:29.868481",
     "exception": false,
     "start_time": "2020-12-01T02:46:29.691708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading training and test sets\n",
    "\n",
    "train_set = pd.read_csv(\"/kaggle/input/aichallenge2020/training.csv\")\n",
    "test_set = pd.read_csv(\"/kaggle/input/aichallenge2020/test.csv\")\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T02:46:29.904440Z",
     "iopub.status.busy": "2020-12-01T02:46:29.903651Z",
     "iopub.status.idle": "2020-12-01T02:46:29.906522Z",
     "shell.execute_reply": "2020-12-01T02:46:29.907112Z"
    },
    "papermill": {
     "duration": 0.023188,
     "end_time": "2020-12-01T02:46:29.907268",
     "exception": false,
     "start_time": "2020-12-01T02:46:29.884080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#display(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015045,
     "end_time": "2020-12-01T02:46:29.937732",
     "exception": false,
     "start_time": "2020-12-01T02:46:29.922687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T02:46:29.996049Z",
     "iopub.status.busy": "2020-12-01T02:46:29.982560Z",
     "iopub.status.idle": "2020-12-01T02:46:30.026353Z",
     "shell.execute_reply": "2020-12-01T02:46:30.025601Z"
    },
    "papermill": {
     "duration": 0.07361,
     "end_time": "2020-12-01T02:46:30.026507",
     "exception": false,
     "start_time": "2020-12-01T02:46:29.952897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Droping duplicated tweets\n",
    "\n",
    "train_set = train_set.drop_duplicates(subset='tweet', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T02:46:30.063057Z",
     "iopub.status.busy": "2020-12-01T02:46:30.062134Z",
     "iopub.status.idle": "2020-12-01T02:46:30.065122Z",
     "shell.execute_reply": "2020-12-01T02:46:30.064356Z"
    },
    "papermill": {
     "duration": 0.023383,
     "end_time": "2020-12-01T02:46:30.065245",
     "exception": false,
     "start_time": "2020-12-01T02:46:30.041862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking for duplicated tweets\n",
    "#train_set.tweet.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T02:46:30.102287Z",
     "iopub.status.busy": "2020-12-01T02:46:30.101539Z",
     "iopub.status.idle": "2020-12-01T02:48:16.745925Z",
     "shell.execute_reply": "2020-12-01T02:48:16.746496Z"
    },
    "papermill": {
     "duration": 106.666006,
     "end_time": "2020-12-01T02:48:16.746694",
     "exception": false,
     "start_time": "2020-12-01T02:46:30.080688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es_core_news_lg==2.3.1\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-2.3.1/es_core_news_lg-2.3.1.tar.gz (573.1 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 573.1 MB 66.2 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from es_core_news_lg==2.3.1) (2.3.2)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (3.0.2)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2.0.3)\r\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (0.4.1)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.0.2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2.23.0)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (0.8.0)\r\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.0.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (4.45.0)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.18.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.0.2)\r\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.1.3)\r\n",
      "Requirement already satisfied: thinc==7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (7.4.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (1.25.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2020.6.20)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (3.0.4)\r\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (2.0.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_lg==2.3.1) (3.1.0)\r\n",
      "Building wheels for collected packages: es-core-news-lg\r\n",
      "  Building wheel for es-core-news-lg (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for es-core-news-lg: filename=es_core_news_lg-2.3.1-py3-none-any.whl size=573139080 sha256=b25588e3105e7f35014166913b354ec6758ae80eaffb009799e7e0de0b17cb20\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lralfrd_/wheels/a5/c7/e8/a49f20406fb1f1f557639d51ce800bdd12241f91e1f21f1e91\r\n",
      "Successfully built es-core-news-lg\r\n",
      "Installing collected packages: es-core-news-lg\r\n",
      "Successfully installed es-core-news-lg-2.3.1\r\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the model via spacy.load('es_core_news_lg')\r\n"
     ]
    }
   ],
   "source": [
    "# Loading pre-trained word embeddings\n",
    "import spacy\n",
    "!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T02:48:17.446326Z",
     "iopub.status.busy": "2020-12-01T02:48:17.445548Z",
     "iopub.status.idle": "2020-12-01T02:48:22.153296Z",
     "shell.execute_reply": "2020-12-01T02:48:22.153865Z"
    },
    "papermill": {
     "duration": 5.063324,
     "end_time": "2020-12-01T02:48:22.154035",
     "exception": false,
     "start_time": "2020-12-01T02:48:17.090711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import es_core_news_lg\n",
    "\n",
    "#____-----____-----____-----____-----____-----____-----____-----____-----\n",
    "\n",
    "def get_embeddings(vocab):\n",
    "    return vocab.vectors.data\n",
    "\n",
    "nlp = es_core_news_lg.load()\n",
    "embeddings = get_embeddings(nlp.vocab)\n",
    "\n",
    "#print(embeddings.shape)\n",
    "\n",
    "# Cleans the tweet\n",
    "def tweet_cleaner(tok_tweet):\n",
    "    tweet = \" \"\n",
    "    tweet = [tweet.join(str(tok.text) for tok in tok_tweet if tok.is_stop==False and tok.like_url==False)]\n",
    "    return tweet\n",
    "#____-----____-----____-----____-----____-----____-----____-----____-----\n",
    "\n",
    "# Preprocesses every tweet with spacy model\n",
    "def preprocess_pipe(set):\n",
    "    tweets = []\n",
    "    for tok_tweet in nlp.pipe(set['tweet'], batch_size=20):\n",
    "        tweets.append(tweet_cleaner(tok_tweet))\n",
    "    tweets = np.array([str(tweet) for tweet in tweets])\n",
    "    return pd.DataFrame({'tweet': tweets})\n",
    "\n",
    "#____-----____-----____-----____-----____-----____-----____-----____-----\n",
    "\n",
    "# Reprocesses clean tweets and vectorizes tweets in order to feed them to the model\n",
    "def tok_vec(set):\n",
    "    #tweets = preprocess_pipe(set)\n",
    "    tok_set = nlp.pipe(set['tweet'], batch_size = 20)\n",
    "    vec_set = get_features(tok_set,25)\n",
    "    return vec_set\n",
    "\n",
    "#____-----____-----____-----____-----____-----____-----____-----____-----\n",
    "\n",
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = np.zeros((len(docs), max_length), dtype=\"int32\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T02:48:22.893513Z",
     "iopub.status.busy": "2020-12-01T02:48:22.892724Z",
     "iopub.status.idle": "2020-12-01T02:51:30.408820Z",
     "shell.execute_reply": "2020-12-01T02:51:30.408123Z"
    },
    "papermill": {
     "duration": 187.906235,
     "end_time": "2020-12-01T02:51:30.408969",
     "exception": false,
     "start_time": "2020-12-01T02:48:22.502734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing train and test sets\n",
    "\n",
    "vec_train_set = tok_vec(train_set)\n",
    "#print(vec_train_set.shape)\n",
    "vec_test_set = tok_vec(test_set)\n",
    "#print(vec_test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T02:51:31.166595Z",
     "iopub.status.busy": "2020-12-01T02:51:31.152417Z",
     "iopub.status.idle": "2020-12-01T02:51:36.324190Z",
     "shell.execute_reply": "2020-12-01T02:51:36.323248Z"
    },
    "papermill": {
     "duration": 5.563816,
     "end_time": "2020-12-01T02:51:36.324340",
     "exception": false,
     "start_time": "2020-12-01T02:51:30.760524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ESTADISTICAS \n",
    "import emoji\n",
    "\n",
    "def cont_signos(tweets):\n",
    "    array=[]\n",
    "    dic={}\n",
    "    for tweet in tweets:\n",
    "        i=0\n",
    "        \n",
    "        for aux2 in tweet:\n",
    "              \n",
    "            if '\\'' in aux2:\n",
    "                i+=1\n",
    "            if '\\\"' in aux2:\n",
    "                i+=1\n",
    "            elif '(' in aux2:\n",
    "                i+=1\n",
    "            elif ')' in aux2:\n",
    "                i+=1\n",
    "            elif '[' in aux2:\n",
    "                i+=1\n",
    "            elif ']' in aux2:\n",
    "                i+=1\n",
    "            elif ':' in aux2:\n",
    "                i+=1\n",
    "            elif '.' in aux2:\n",
    "                i+=1\n",
    "            elif ',' in aux2:\n",
    "                i+=1\n",
    "            elif '¿' in aux2:\n",
    "                i+=1\n",
    "            elif '?' in aux2:\n",
    "                i+=1\n",
    "            elif '¡' in aux2:\n",
    "                i+=1\n",
    "            elif '!' in aux2:\n",
    "                i+=1\n",
    "            elif ';' in aux2:\n",
    "                i+=1\n",
    "            elif '{' in aux2:\n",
    "                i+=1\n",
    "            elif '}' in aux2:\n",
    "                i+=1\n",
    "            elif '...' in aux2:\n",
    "                i+=1\n",
    "            elif '-' in aux2:\n",
    "                i+=1\n",
    "        array.append(i)\n",
    "    dic['signos']=array \n",
    "    return(dic)\n",
    "\n",
    "cantsignos_train=pd.DataFrame(data=cont_signos(train_set['tweet']))\n",
    "cantsignos_test=pd.DataFrame(data=cont_signos(test_set['tweet']))\n",
    "\n",
    "def veri_comillas(tweets):\n",
    "    array=[]\n",
    "    dic={}\n",
    "\n",
    "    for tweet in tweets:\n",
    "        i=0\n",
    "        if '\\\"' in tweet:\n",
    "            i=1\n",
    "        array.append(i)\n",
    "    dic[\"veri_comillas\"]=array\n",
    "    return(dic)\n",
    "\n",
    "comillas_train=pd.DataFrame(data=veri_comillas(train_set['tweet']))\n",
    "comillas_test=pd.DataFrame(data=veri_comillas(test_set['tweet']))\n",
    "\n",
    "def contar_arrobas(tweets):\n",
    "    lis=[]\n",
    "    arrobas_dic={}\n",
    "    for tweet in tweets:\n",
    "        \n",
    "        aux=tweet.split()\n",
    "        \n",
    "        i=0\n",
    "        \n",
    "        for extraer in aux:\n",
    "          \n",
    "            \n",
    "            if '@' in extraer:\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "        lis.append(i)\n",
    "    arrobas_dic[\"cant_arrobas\"]=lis       \n",
    "    return(arrobas_dic)     \n",
    "               \n",
    "        \n",
    "count_arrobas=pd.DataFrame(data=contar_arrobas(train_set[\"tweet\"]))\n",
    "count_arrobas_test=pd.DataFrame(data=contar_arrobas(test_set[\"tweet\"]))\n",
    "\n",
    "def contar_hashtag(tweets):\n",
    "    hash_dic={}\n",
    "    lis=[]\n",
    "    for tweet in tweets:\n",
    "        \n",
    "        aux=tweet.split()\n",
    "        \n",
    "        i=0\n",
    "        \n",
    "        \n",
    "        for extraer in aux:\n",
    "            if '#' in extraer:\n",
    "                i+=1\n",
    "               \n",
    "        lis.append(i)\n",
    "    \n",
    "    hash_dic[\"cant_hash\"]=lis\n",
    "    return(hash_dic)  \n",
    "               \n",
    "count_hashtag=pd.DataFrame(data=contar_hashtag(train_set[\"tweet\"]))\n",
    "count_hashtag_test=pd.DataFrame(data=contar_hashtag(test_set[\"tweet\"]))\n",
    "\n",
    "def contar_emoji(tweets):\n",
    "    emoji_dic={}\n",
    "    auxi=[]\n",
    "    for tweet in tweets:\n",
    "        i=0\n",
    "      \n",
    "        aux=tweet.split()\n",
    "       \n",
    "        for extraer in aux:\n",
    "            for e in extraer:\n",
    "              \n",
    "                if e in emoji.UNICODE_EMOJI:\n",
    "                    \n",
    "                    i+=1\n",
    "            \n",
    "    \n",
    "        auxi.append(i)\n",
    "               \n",
    "    emoji_dic['cant_emoji']=auxi\n",
    "    return (emoji_dic)\n",
    "count_emoji=pd.DataFrame(data=contar_emoji(train_set['tweet']))\n",
    "c_emoji_test=pd.DataFrame(data=contar_emoji(test_set['tweet']))\n",
    "\n",
    "def count_up_case(tweets):\n",
    "    up_cases={}\n",
    "    up_cases_list=[]\n",
    "    for tweet in tweets:\n",
    "        i=0\n",
    "        split_tweet=tweet.split()\n",
    "        for word in split_tweet:\n",
    "            if word.isupper():\n",
    "                i+=1\n",
    "        up_cases_list.append(i)\n",
    "    up_cases['num_up_cases']=up_cases_list\n",
    "    return (up_cases)\n",
    "num_up_cases_train=pd.DataFrame(data=count_up_case(train_set['tweet']))\n",
    "num_up_cases_test=pd.DataFrame(data=count_up_case(test_set['tweet']))\n",
    "\n",
    "def non_vocab_token(tweets):\n",
    "    n_vocab_t={}\n",
    "    n_vocab_list=[]\n",
    "    for tweet in tweets:\n",
    "        i=0\n",
    "        split_tweet=tweet.split()\n",
    "        for token in split_tweet:\n",
    "            token = nlp.vocab.strings[token]\n",
    "            if token not in nlp.vocab:\n",
    "                i+=1\n",
    "        n_vocab_list.append(i)\n",
    "    n_vocab_t['num_n_vocab_t']=n_vocab_list\n",
    "    return n_vocab_t\n",
    "num_n_vocab_t_train=pd.DataFrame(data=non_vocab_token(train_set['tweet']))\n",
    "num_n_vocab_t_test=pd.DataFrame(data=non_vocab_token(test_set['tweet']))\n",
    "\n",
    "\n",
    "stadis=pd.concat([count_hashtag,num_up_cases_train,num_n_vocab_t_train,cantsignos_train,count_arrobas],axis=1)\n",
    "stadis_test=pd.concat([count_hashtag_test,num_up_cases_test,num_n_vocab_t_test,cantsignos_test,count_arrobas_test],axis=1)\n",
    "#print(stadis)\n",
    "stadis=np.array(stadis)\n",
    "stadis_test=np.array(stadis_test)\n",
    "stadis_norm = np.linalg.norm(stadis)\n",
    "stadis = stadis/stadis_norm\n",
    "stadis_test_norm = np.linalg.norm(stadis_test)\n",
    "stadis_test= stadis_test/stadis_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T02:51:37.000884Z",
     "iopub.status.busy": "2020-12-01T02:51:37.000092Z",
     "iopub.status.idle": "2020-12-01T02:51:38.144002Z",
     "shell.execute_reply": "2020-12-01T02:51:38.143098Z"
    },
    "papermill": {
     "duration": 1.483767,
     "end_time": "2020-12-01T02:51:38.144183",
     "exception": false,
     "start_time": "2020-12-01T02:51:36.660416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting train and val set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    " \n",
    "X_train2, X_val2,y_train2,y_val2= train_test_split(stadis, train_set['is_organico'], test_size=0.2,stratify=train_set['is_organico']) \n",
    "\n",
    "X_train, X_val, y_train, y_val= train_test_split(vec_train_set,train_set['is_organico'], test_size=0.2,stratify=train_set['is_organico'])\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_val.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_val.shape)\n",
    "\n",
    "#print(\" \")\n",
    "\n",
    "#print(X_train2.shape)\n",
    "#print(X_val2.shape)\n",
    "#print(y_train2.shape)\n",
    "#print(y_val2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T02:51:38.872587Z",
     "iopub.status.busy": "2020-12-01T02:51:38.870146Z",
     "iopub.status.idle": "2020-12-01T02:51:44.516125Z",
     "shell.execute_reply": "2020-12-01T02:51:44.515103Z"
    },
    "papermill": {
     "duration": 6.00208,
     "end_time": "2020-12-01T02:51:44.516286",
     "exception": false,
     "start_time": "2020-12-01T02:51:38.514206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T02:51:45.217691Z",
     "iopub.status.busy": "2020-12-01T02:51:45.216790Z",
     "iopub.status.idle": "2020-12-01T02:51:49.112723Z",
     "shell.execute_reply": "2020-12-01T02:51:49.111873Z"
    },
    "papermill": {
     "duration": 4.260002,
     "end_time": "2020-12-01T02:51:49.112872",
     "exception": false,
     "start_time": "2020-12-01T02:51:44.852870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Architecture\n",
    "\n",
    "import keras\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Flatten,ReLU, Concatenate, Conv1D,Dropout,LeakyReLU\n",
    "from keras.layers import BatchNormalization,Embedding,Concatenate,SpatialDropout1D,GaussianDropout,LayerNormalization\n",
    "from keras.layers import GlobalAveragePooling1D,GlobalMaxPooling1D\n",
    "\n",
    "model1 = Input(shape=(25,))\n",
    "model2 = Input(shape=(5,))\n",
    "\n",
    "x=Embedding(embeddings.shape[0],\n",
    "                    embeddings.shape[1],\n",
    "                    input_length=25,\n",
    "                    trainable=False,\n",
    "                    weights=[embeddings],\n",
    "                    mask_zero=True)(model1)\n",
    "\n",
    "\n",
    "x=Conv1D(64,1)(x)\n",
    "x=Bidirectional(LSTM(128, return_sequences = True, go_backwards = True))(x)\n",
    "x=SpatialDropout1D(0.2)(x)\n",
    "x=Bidirectional(LSTM(64, return_sequences = True, go_backwards = True))(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=Dense(256, activation='relu')(x)\n",
    "\n",
    "#print(x)\n",
    "att = Attention(25)(x)\n",
    "avg_pool1 =GlobalAveragePooling1D()(x)\n",
    "max_pool1 =GlobalMaxPooling1D()(x)\n",
    "x=Concatenate()([att,avg_pool1, max_pool1])\n",
    "\n",
    "y=Dense(128, activation='relu')(model2)\n",
    "y=Dropout(0.2)(y)\n",
    "y=Dense(100, activation='relu')(y)\n",
    "y=Dropout(0.2)(y)\n",
    "model_concat = Concatenate()([x,y])\n",
    "\n",
    "\n",
    "model_concat=Dense(64, activation='relu')(model_concat)\n",
    "model_concat=Dense(1, activation='sigmoid')(model_concat)\n",
    "\n",
    "\n",
    "model=Model(inputs=[model1,model2], outputs=model_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T02:51:49.800449Z",
     "iopub.status.busy": "2020-12-01T02:51:49.799279Z",
     "iopub.status.idle": "2020-12-01T03:05:41.913547Z",
     "shell.execute_reply": "2020-12-01T03:05:41.912766Z"
    },
    "papermill": {
     "duration": 832.469666,
     "end_time": "2020-12-01T03:05:41.913695",
     "exception": false,
     "start_time": "2020-12-01T02:51:49.444029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "171/171 [==============================] - 28s 164ms/step - loss: 0.4836 - acc: 0.7633 - val_loss: 0.4860 - val_acc: 0.8090\n",
      "Epoch 2/30\n",
      "171/171 [==============================] - 27s 156ms/step - loss: 0.3675 - acc: 0.8280 - val_loss: 0.3799 - val_acc: 0.8337\n",
      "Epoch 3/30\n",
      "171/171 [==============================] - 27s 156ms/step - loss: 0.2973 - acc: 0.8664 - val_loss: 0.3351 - val_acc: 0.8490\n",
      "Epoch 4/30\n",
      "171/171 [==============================] - 27s 159ms/step - loss: 0.2484 - acc: 0.8927 - val_loss: 0.3079 - val_acc: 0.8611\n",
      "Epoch 5/30\n",
      "171/171 [==============================] - 27s 157ms/step - loss: 0.2039 - acc: 0.9131 - val_loss: 0.3139 - val_acc: 0.8685\n",
      "Epoch 6/30\n",
      "171/171 [==============================] - 28s 162ms/step - loss: 0.1677 - acc: 0.9296 - val_loss: 0.3096 - val_acc: 0.8726\n",
      "Epoch 7/30\n",
      "171/171 [==============================] - 27s 159ms/step - loss: 0.1391 - acc: 0.9432 - val_loss: 0.3407 - val_acc: 0.8756\n",
      "Epoch 8/30\n",
      "171/171 [==============================] - 28s 164ms/step - loss: 0.1140 - acc: 0.9566 - val_loss: 0.4023 - val_acc: 0.8535\n",
      "Epoch 9/30\n",
      "171/171 [==============================] - 28s 166ms/step - loss: 0.0949 - acc: 0.9640 - val_loss: 0.3754 - val_acc: 0.8779\n",
      "Epoch 10/30\n",
      "171/171 [==============================] - 26s 155ms/step - loss: 0.0825 - acc: 0.9666 - val_loss: 0.3938 - val_acc: 0.8775\n",
      "Epoch 11/30\n",
      "171/171 [==============================] - 27s 160ms/step - loss: 0.0751 - acc: 0.9699 - val_loss: 0.4196 - val_acc: 0.8758\n",
      "Epoch 12/30\n",
      "171/171 [==============================] - 27s 157ms/step - loss: 0.0659 - acc: 0.9745 - val_loss: 0.3911 - val_acc: 0.8803\n",
      "Epoch 13/30\n",
      "171/171 [==============================] - 27s 159ms/step - loss: 0.0569 - acc: 0.9784 - val_loss: 0.4799 - val_acc: 0.8830\n",
      "Epoch 14/30\n",
      "171/171 [==============================] - 27s 158ms/step - loss: 0.0537 - acc: 0.9802 - val_loss: 0.4528 - val_acc: 0.8750\n",
      "Epoch 15/30\n",
      "171/171 [==============================] - 28s 163ms/step - loss: 0.0483 - acc: 0.9820 - val_loss: 0.4639 - val_acc: 0.8734\n",
      "Epoch 16/30\n",
      "171/171 [==============================] - 27s 157ms/step - loss: 0.0405 - acc: 0.9849 - val_loss: 0.5234 - val_acc: 0.8783\n",
      "Epoch 17/30\n",
      "171/171 [==============================] - 27s 158ms/step - loss: 0.0401 - acc: 0.9849 - val_loss: 0.5141 - val_acc: 0.8857\n",
      "Epoch 18/30\n",
      "171/171 [==============================] - 28s 163ms/step - loss: 0.0408 - acc: 0.9846 - val_loss: 0.5068 - val_acc: 0.8736\n",
      "Epoch 19/30\n",
      "171/171 [==============================] - 27s 159ms/step - loss: 0.0375 - acc: 0.9863 - val_loss: 0.5633 - val_acc: 0.8738\n",
      "Epoch 20/30\n",
      "171/171 [==============================] - 27s 161ms/step - loss: 0.0436 - acc: 0.9833 - val_loss: 0.4933 - val_acc: 0.8816\n",
      "Epoch 21/30\n",
      "171/171 [==============================] - 27s 159ms/step - loss: 0.0364 - acc: 0.9868 - val_loss: 0.5022 - val_acc: 0.8826\n",
      "Epoch 22/30\n",
      "171/171 [==============================] - 28s 162ms/step - loss: 0.0343 - acc: 0.9871 - val_loss: 0.5419 - val_acc: 0.8818\n",
      "Epoch 23/30\n",
      "171/171 [==============================] - 26s 155ms/step - loss: 0.0306 - acc: 0.9888 - val_loss: 0.5409 - val_acc: 0.8826\n",
      "Epoch 24/30\n",
      "171/171 [==============================] - 28s 162ms/step - loss: 0.0354 - acc: 0.9866 - val_loss: 0.5209 - val_acc: 0.8816\n",
      "Epoch 25/30\n",
      "171/171 [==============================] - 27s 158ms/step - loss: 0.0354 - acc: 0.9875 - val_loss: 0.4734 - val_acc: 0.8763\n",
      "Epoch 26/30\n",
      "171/171 [==============================] - 28s 164ms/step - loss: 0.0306 - acc: 0.9890 - val_loss: 0.5392 - val_acc: 0.8838\n",
      "Epoch 27/30\n",
      "171/171 [==============================] - 27s 157ms/step - loss: 0.0260 - acc: 0.9907 - val_loss: 0.5532 - val_acc: 0.8787\n",
      "Epoch 28/30\n",
      "171/171 [==============================] - 29s 167ms/step - loss: 0.0340 - acc: 0.9880 - val_loss: 0.5109 - val_acc: 0.8740\n",
      "Epoch 29/30\n",
      "171/171 [==============================] - 27s 158ms/step - loss: 0.0273 - acc: 0.9911 - val_loss: 0.5397 - val_acc: 0.8846\n",
      "Epoch 30/30\n",
      "171/171 [==============================] - 28s 164ms/step - loss: 0.0275 - acc: 0.9902 - val_loss: 0.5201 - val_acc: 0.8846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa420284a90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=1e-3)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"acc\"])\n",
    "\n",
    "model.fit([X_train,X_train2],y_train, batch_size=120, epochs=30, validation_data=([X_val,X_val2], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-01T03:05:46.816505Z",
     "iopub.status.busy": "2020-12-01T03:05:46.815638Z",
     "iopub.status.idle": "2020-12-01T03:05:54.515933Z",
     "shell.execute_reply": "2020-12-01T03:05:54.515209Z"
    },
    "papermill": {
     "duration": 10.125411,
     "end_time": "2020-12-01T03:05:54.516067",
     "exception": false,
     "start_time": "2020-12-01T03:05:44.390656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "\n",
    "predicted_prob = model.predict([vec_test_set,stadis_test]) #Computes probability of each tweet being organic\n",
    "predictions = np.round(predicted_prob).astype(int).flatten() # Rounds the probability to 0/1\n",
    "submission = pd.DataFrame({'Id': test_set['Id'],'Predicted': predictions})\n",
    "submission.to_csv('submission.csv', index=False) #Turns dataframe into csv file\n",
    "#display(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1172.663665,
   "end_time": "2020-12-01T03:05:57.179781",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-01T02:46:24.516116",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
